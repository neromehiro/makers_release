/api/__init__.py:
--------------------------------------------------------------------------------
  1 | # Makes api a package for local imports.
--------------------------------------------------------------------------------

/api/a1_check_releace.py:
--------------------------------------------------------------------------------
  1 | import json
  2 | import re
  3 | import sys
  4 | import xml.etree.ElementTree as ET
  5 | from datetime import datetime, timedelta, timezone
  6 | from pathlib import Path
  7 | from typing import Any, Dict, List
  8 | 
  9 | import requests
 10 | 
 11 | BASE_DIR = Path(__file__).resolve().parent.parent
 12 | if str(BASE_DIR) not in sys.path:
 13 |     sys.path.append(str(BASE_DIR))
 14 | 
 15 | from spreadsheet2json import load_spreadsheet_data
 16 | 
 17 | SITEMAP_URL = "https://prtimes.jp/sitemap-news.xml"
 18 | OUTPUT_DIR = Path(__file__).resolve().parent.parent / "output"
 19 | OUTPUT_PATH = OUTPUT_DIR / "a1_check_releace.json"
 20 | 
 21 | 
 22 | def load_prtimes_ids() -> List[str]:
 23 |     """Load company_id list from spreadsheet2json output."""
 24 |     data = load_spreadsheet_data(persist=False)
 25 |     raw_ids = data.get("prtimes_id", []) if isinstance(data, dict) else []
 26 |     ids: List[str] = []
 27 |     for cid in raw_ids:
 28 |         cid_str = str(cid).strip()
 29 |         if not cid_str:
 30 |             continue
 31 |         ids.append(cid_str.lstrip("0") or "0")
 32 |     return ids
 33 | 
 34 | 
 35 | def fetch_sitemap() -> bytes:
 36 |     """Fetch the news sitemap (contains latest press releases across PR TIMES)."""
 37 |     resp = requests.get(SITEMAP_URL, timeout=15)
 38 |     resp.raise_for_status()
 39 |     return resp.content
 40 | 
 41 | 
 42 | def parse_sitemap(xml_bytes: bytes) -> List[Dict[str, Any]]:
 43 |     """Parse sitemap XML and return list of releases with company_id."""
 44 |     ns = {
 45 |         "sm": "http://www.sitemaps.org/schemas/sitemap/0.9",
 46 |         "news": "http://www.google.com/schemas/sitemap-news/0.9",
 47 |     }
 48 |     root = ET.fromstring(xml_bytes)
 49 |     releases: List[Dict[str, Any]] = []
 50 |     pattern = re.compile(r"/p/(\d+)\.(\d+)\.html")
 51 | 
 52 |     for url_node in root.findall("sm:url", ns):
 53 |         loc_el = url_node.find("sm:loc", ns)
 54 |         news_el = url_node.find("news:news", ns)
 55 |         if loc_el is None or news_el is None:
 56 |             continue
 57 | 
 58 |         loc = (loc_el.text or "").strip()
 59 |         m = pattern.search(loc)
 60 |         if not m:
 61 |             continue
 62 | 
 63 |         company_id = m.group(2).lstrip("0") or "0"
 64 |         pub_date_el = news_el.find("news:publication_date", ns)
 65 |         title_el = news_el.find("news:title", ns)
 66 |         if pub_date_el is None:
 67 |             continue
 68 | 
 69 |         pub_raw = (pub_date_el.text or "").strip()
 70 |         try:
 71 |             published_at = datetime.fromisoformat(pub_raw)
 72 |         except ValueError:
 73 |             continue
 74 | 
 75 |         releases.append(
 76 |             {
 77 |                 "company_id": company_id,
 78 |                 "url": loc,
 79 |                 "title": (title_el.text or "").strip() if title_el is not None else "",
 80 |                 "published_at": published_at,
 81 |             }
 82 |         )
 83 |     return releases
 84 | 
 85 | 
 86 | def filter_recent_releases(
 87 |     releases: List[Dict[str, Any]],
 88 |     target_ids: List[str],
 89 |     window_hours: int = 1,
 90 | ) -> List[Dict[str, Any]]:
 91 |     """Filter releases to target company IDs within the last window_hours."""
 92 |     target_set = set(target_ids)
 93 |     now = datetime.now(timezone.utc)
 94 |     window_start = now - timedelta(hours=window_hours)
 95 |     filtered: List[Dict[str, Any]] = []
 96 | 
 97 |     for r in releases:
 98 |         cid = r["company_id"]
 99 |         pub_dt: datetime = r["published_at"]
100 |         pub_dt_utc = pub_dt.astimezone(timezone.utc)
101 | 
102 |         if cid not in target_set:
103 |             continue
104 |         if pub_dt_utc < window_start:
105 |             continue
106 | 
107 |         filtered.append(
108 |             {
109 |                 "company_id": cid,
110 |                 "url": r["url"],
111 |                 "title": r["title"],
112 |                 "published_at": pub_dt_utc.isoformat(),
113 |             }
114 |         )
115 | 
116 |     filtered.sort(key=lambda x: x["published_at"], reverse=True)
117 |     return filtered
118 | 
119 | 
120 | def check_releases(window_hours: int = 1) -> Dict[str, Any]:
121 |     """High-level entry: fetch sitemap, filter, and return structured data."""
122 |     ids = load_prtimes_ids()
123 |     xml_bytes = fetch_sitemap()
124 |     releases = parse_sitemap(xml_bytes)
125 |     recent = filter_recent_releases(releases, ids, window_hours=window_hours)
126 | 
127 |     now = datetime.now(timezone.utc)
128 |     payload: Dict[str, Any] = {
129 |         "checked_at": now.isoformat(),
130 |         "window_hours": window_hours,
131 |         "target_ids": ids,
132 |         "count": len(recent),
133 |         "releases": recent,
134 |     }
135 |     return payload
136 | 
137 | 
138 | def write_output(data: Dict[str, Any]) -> None:
139 |     try:
140 |         OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
141 |         OUTPUT_PATH.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")
142 |     except OSError:
143 |         # Skip persist on read-only filesystems (e.g., serverless)
144 |         pass
145 | 
146 | 
147 | def main():
148 |     data = check_releases(window_hours=1)
149 |     write_output(data)
150 |     print(f"Saved {data['count']} releases to {OUTPUT_PATH}")
151 | 
152 | 
153 | if __name__ == "__main__":
154 |     main()
--------------------------------------------------------------------------------

/api/a2_check_note.py:
--------------------------------------------------------------------------------
  1 | import json
  2 | import sys
  3 | import xml.etree.ElementTree as ET
  4 | from datetime import datetime, timedelta, timezone
  5 | from email.utils import parsedate_to_datetime
  6 | from pathlib import Path
  7 | from typing import Any, Dict, List
  8 | from urllib.parse import urlparse
  9 | 
 10 | import requests
 11 | 
 12 | BASE_DIR = Path(__file__).resolve().parent.parent
 13 | if str(BASE_DIR) not in sys.path:
 14 |     sys.path.append(str(BASE_DIR))
 15 | 
 16 | from spreadsheet2json import load_spreadsheet_data
 17 | 
 18 | OUTPUT_DIR = Path(__file__).resolve().parent.parent / "output"
 19 | OUTPUT_PATH = OUTPUT_DIR / "a2_check_note.py.json"
 20 | FEED_URL_TEMPLATE = "https://note.com/{note_id}/rss"
 21 | HTTP_HEADERS = {
 22 |     "User-Agent": (
 23 |         "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
 24 |         "AppleWebKit/537.36 (KHTML, like Gecko) "
 25 |         "Chrome/124.0.0.0 Safari/537.36"
 26 |     )
 27 | }
 28 | 
 29 | 
 30 | def _normalize_note_id(value: str) -> str:
 31 |     """Extract note username/slug from URL or raw value."""
 32 |     raw = value.strip()
 33 |     if not raw:
 34 |         return ""
 35 | 
 36 |     if "note.com" in raw or "://" in raw:
 37 |         parsed = urlparse(raw if "://" in raw else f"https://{raw}")
 38 |         path = parsed.path.strip("/")
 39 |         if not path:
 40 |             return ""
 41 |         raw = path.split("/")[0]
 42 | 
 43 |     return raw.lstrip("@")
 44 | 
 45 | 
 46 | def load_note_ids() -> List[str]:
 47 |     """Load note user IDs from spreadsheet2json output."""
 48 |     data = load_spreadsheet_data(persist=False)
 49 |     raw_ids = data.get("note_id", []) if isinstance(data, dict) else []
 50 |     ids: List[str] = []
 51 |     for note_id in raw_ids:
 52 |         normalized = _normalize_note_id(str(note_id))
 53 |         if normalized:
 54 |             ids.append(normalized)
 55 |     return ids
 56 | 
 57 | 
 58 | def fetch_rss(note_id: str) -> bytes:
 59 |     """Fetch RSS feed bytes for a given note user."""
 60 |     url = FEED_URL_TEMPLATE.format(note_id=note_id)
 61 |     resp = requests.get(url, headers=HTTP_HEADERS, timeout=15)
 62 |     resp.raise_for_status()
 63 |     return resp.content
 64 | 
 65 | 
 66 | def parse_rss(xml_bytes: bytes, note_id: str) -> List[Dict[str, Any]]:
 67 |     """Parse RSS XML and return articles for the specified note_id."""
 68 |     try:
 69 |         root = ET.fromstring(xml_bytes)
 70 |     except ET.ParseError:
 71 |         return []
 72 | 
 73 |     channel = root.find("channel")
 74 |     if channel is None:
 75 |         return []
 76 | 
 77 |     items: List[Dict[str, Any]] = []
 78 |     for item in channel.findall("item"):
 79 |         title = (item.findtext("title") or "").strip()
 80 |         link = (item.findtext("link") or "").strip()
 81 |         pub_raw = (item.findtext("pubDate") or "").strip()
 82 |         if not link or not pub_raw:
 83 |             continue
 84 | 
 85 |         try:
 86 |             pub_dt = parsedate_to_datetime(pub_raw)
 87 |         except (TypeError, ValueError):
 88 |             continue
 89 | 
 90 |         if pub_dt.tzinfo is None:
 91 |             pub_dt = pub_dt.replace(tzinfo=timezone.utc)
 92 | 
 93 |         items.append(
 94 |             {
 95 |                 "note_id": note_id,
 96 |                 "url": link,
 97 |                 "title": title,
 98 |                 "published_at": pub_dt,
 99 |             }
100 |         )
101 |     return items
102 | 
103 | 
104 | def filter_recent(articles: List[Dict[str, Any]], window_hours: int = 1) -> List[Dict[str, Any]]:
105 |     """Return articles published within the last window_hours."""
106 |     now = datetime.now(timezone.utc)
107 |     window_start = now - timedelta(hours=window_hours)
108 | 
109 |     filtered: List[Dict[str, Any]] = []
110 |     for article in articles:
111 |         pub_dt: datetime = article["published_at"]
112 |         pub_dt_utc = pub_dt.astimezone(timezone.utc)
113 |         if pub_dt_utc < window_start:
114 |             continue
115 | 
116 |         filtered.append(
117 |             {
118 |                 "note_id": article["note_id"],
119 |                 "url": article["url"],
120 |                 "title": article["title"],
121 |                 "published_at": pub_dt_utc.isoformat(),
122 |             }
123 |         )
124 | 
125 |     filtered.sort(key=lambda x: x["published_at"], reverse=True)
126 |     return filtered
127 | 
128 | 
129 | def check_notes(window_hours: int = 1) -> Dict[str, Any]:
130 |     """High-level entry: fetch RSS feeds, filter recent, and return structured data."""
131 |     note_ids = load_note_ids()
132 |     all_articles: List[Dict[str, Any]] = []
133 | 
134 |     for note_id in note_ids:
135 |         try:
136 |             xml_bytes = fetch_rss(note_id)
137 |             all_articles.extend(parse_rss(xml_bytes, note_id))
138 |         except Exception:
139 |             continue
140 | 
141 |     recent = filter_recent(all_articles, window_hours=window_hours)
142 |     now = datetime.now(timezone.utc)
143 |     payload: Dict[str, Any] = {
144 |         "checked_at": now.isoformat(),
145 |         "window_hours": window_hours,
146 |         "target_ids": note_ids,
147 |         "count": len(recent),
148 |         "articles": recent,
149 |     }
150 |     return payload
151 | 
152 | 
153 | def write_output(data: Dict[str, Any]) -> None:
154 |     try:
155 |         OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
156 |         OUTPUT_PATH.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")
157 |     except OSError:
158 |         # Skip persist on read-only filesystems (e.g., serverless)
159 |         pass
160 | 
161 | 
162 | def main():
163 |     data = check_notes(window_hours=1)
164 |     write_output(data)
165 |     print(f"Saved {data['count']} articles to {OUTPUT_PATH}")
166 | 
167 | 
168 | if __name__ == "__main__":
169 |     main()
--------------------------------------------------------------------------------

/api/a3_check_x.py:
--------------------------------------------------------------------------------
  1 | import json
  2 | import sys
  3 | import xml.etree.ElementTree as ET
  4 | from datetime import datetime, timedelta, timezone
  5 | from email.utils import parsedate_to_datetime
  6 | from pathlib import Path
  7 | from typing import Any, Dict, List
  8 | 
  9 | import requests
 10 | 
 11 | BASE_DIR = Path(__file__).resolve().parent.parent
 12 | if str(BASE_DIR) not in sys.path:
 13 |     sys.path.append(str(BASE_DIR))
 14 | 
 15 | from spreadsheet2json import load_spreadsheet_data
 16 | 
 17 | OUTPUT_DIR = Path(__file__).resolve().parent.parent / "output"
 18 | OUTPUT_PATH = OUTPUT_DIR / "a3_check_x.json"
 19 | FEED_URL_TEMPLATE = "https://nitter.net/{x_id}/rss"
 20 | HTTP_HEADERS = {
 21 |     "User-Agent": (
 22 |         "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
 23 |         "AppleWebKit/537.36 (KHTML, like Gecko) "
 24 |         "Chrome/124.0.0.0 Safari/537.36"
 25 |     )
 26 | }
 27 | 
 28 | 
 29 | def load_x_ids() -> List[str]:
 30 |     """Load X(Twitter) user IDs from spreadsheet2json output."""
 31 |     data = load_spreadsheet_data(persist=False)
 32 |     raw_ids = data.get("x_id", []) if isinstance(data, dict) else []
 33 |     ids: List[str] = []
 34 |     for x_id in raw_ids:
 35 |         x_str = str(x_id).strip()
 36 |         if x_str:
 37 |             ids.append(x_str)
 38 |     return ids
 39 | 
 40 | 
 41 | def fetch_rss(x_id: str) -> bytes:
 42 |     """Fetch RSS feed bytes for a given X user via Nitter."""
 43 |     url = FEED_URL_TEMPLATE.format(x_id=x_id)
 44 |     resp = requests.get(url, headers=HTTP_HEADERS, timeout=15)
 45 |     resp.raise_for_status()
 46 |     return resp.content
 47 | 
 48 | 
 49 | def parse_rss(xml_bytes: bytes, x_id: str) -> List[Dict[str, Any]]:
 50 |     """Parse RSS XML and return tweets for the specified x_id."""
 51 |     try:
 52 |         root = ET.fromstring(xml_bytes)
 53 |     except ET.ParseError:
 54 |         return []
 55 | 
 56 |     channel = root.find("channel")
 57 |     if channel is None:
 58 |         return []
 59 | 
 60 |     tweets: List[Dict[str, Any]] = []
 61 |     for item in channel.findall("item"):
 62 |         title = (item.findtext("title") or "").strip()
 63 |         link = (item.findtext("link") or "").strip()
 64 |         pub_raw = (item.findtext("pubDate") or "").strip()
 65 |         if not link or not pub_raw:
 66 |             continue
 67 | 
 68 |         try:
 69 |             pub_dt = parsedate_to_datetime(pub_raw)
 70 |         except (TypeError, ValueError):
 71 |             continue
 72 | 
 73 |         if pub_dt.tzinfo is None:
 74 |             pub_dt = pub_dt.replace(tzinfo=timezone.utc)
 75 | 
 76 |         tweets.append(
 77 |             {
 78 |                 "x_id": x_id,
 79 |                 "url": link,
 80 |                 "title": title,
 81 |                 "published_at": pub_dt,
 82 |             }
 83 |         )
 84 |     return tweets
 85 | 
 86 | 
 87 | def filter_recent(tweets: List[Dict[str, Any]], window_hours: int = 1) -> List[Dict[str, Any]]:
 88 |     """Return tweets published within the last window_hours."""
 89 |     now = datetime.now(timezone.utc)
 90 |     window_start = now - timedelta(hours=window_hours)
 91 | 
 92 |     filtered: List[Dict[str, Any]] = []
 93 |     for tweet in tweets:
 94 |         pub_dt: datetime = tweet["published_at"]
 95 |         pub_dt_utc = pub_dt.astimezone(timezone.utc)
 96 |         if pub_dt_utc < window_start:
 97 |             continue
 98 | 
 99 |         filtered.append(
100 |             {
101 |                 "x_id": tweet["x_id"],
102 |                 "url": tweet["url"],
103 |                 "title": tweet["title"],
104 |                 "published_at": pub_dt_utc.isoformat(),
105 |             }
106 |         )
107 | 
108 |     filtered.sort(key=lambda x: x["published_at"], reverse=True)
109 |     return filtered
110 | 
111 | 
112 | def check_x(window_hours: int = 1) -> Dict[str, Any]:
113 |     """High-level entry: fetch Nitter feeds, filter recent, and return structured data."""
114 |     x_ids = load_x_ids()
115 |     all_tweets: List[Dict[str, Any]] = []
116 | 
117 |     for x_id in x_ids:
118 |         try:
119 |             xml_bytes = fetch_rss(x_id)
120 |             all_tweets.extend(parse_rss(xml_bytes, x_id))
121 |         except Exception:
122 |             continue
123 | 
124 |     recent = filter_recent(all_tweets, window_hours=window_hours)
125 |     now = datetime.now(timezone.utc)
126 |     payload: Dict[str, Any] = {
127 |         "checked_at": now.isoformat(),
128 |         "window_hours": window_hours,
129 |         "target_ids": x_ids,
130 |         "count": len(recent),
131 |         "tweets": recent,
132 |     }
133 |     return payload
134 | 
135 | 
136 | def write_output(data: Dict[str, Any]) -> None:
137 |     try:
138 |         OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
139 |         OUTPUT_PATH.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")
140 |     except OSError:
141 |         # Skip persist on read-only filesystems (e.g., serverless)
142 |         pass
143 | 
144 | 
145 | def main():
146 |     data = check_x(window_hours=1)
147 |     write_output(data)
148 |     print(f"Saved {data['count']} tweets to {OUTPUT_PATH}")
149 | 
150 | 
151 | if __name__ == "__main__":
152 |     main()
--------------------------------------------------------------------------------

/api/b_send_slack.py:
--------------------------------------------------------------------------------
  1 | import os
  2 | import json
  3 | import sys
  4 | from datetime import datetime, timedelta, timezone
  5 | from pathlib import Path
  6 | from typing import Any, Dict, List, Optional
  7 | import re
  8 | from http.server import BaseHTTPRequestHandler
  9 | 
 10 | import requests
 11 | 
 12 | BASE_DIR = Path(__file__).resolve().parent.parent
 13 | if str(BASE_DIR) not in sys.path:
 14 |     sys.path.append(str(BASE_DIR))
 15 | 
 16 | import api.a2_check_note as note_checker
 17 | import api.a1_check_releace as pr_checker
 18 | from spreadsheet2json import load_spreadsheet_data
 19 | 
 20 | 
 21 | def _load_env_file():
 22 |     """
 23 |     Minimal .env loader for local runs (KEY="value" lines only).
 24 |     Safe to call in any environment.
 25 |     """
 26 |     env_path = Path(__file__).resolve().parent.parent / ".env"
 27 |     if not env_path.exists():
 28 |         return
 29 | 
 30 |     with env_path.open(encoding="utf-8") as f:
 31 |         for line in f:
 32 |             line = line.strip()
 33 |             if not line or line.startswith("#") or "=" not in line:
 34 |                 continue
 35 |             key, val = line.split("=", 1)
 36 |             val = val.strip().strip('"').strip("'")
 37 |             os.environ.setdefault(key, val)
 38 | 
 39 | 
 40 | _load_env_file()
 41 | 
 42 | SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL")
 43 | HTTP_HEADERS = {
 44 |     "User-Agent": (
 45 |         "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
 46 |         "AppleWebKit/537.36 (KHTML, like Gecko) "
 47 |         "Chrome/124.0.0.0 Safari/537.36"
 48 |     )
 49 | }
 50 | 
 51 | 
 52 | def send_to_slack(
 53 |     text: str,
 54 |     *,
 55 |     enable_unfurl: bool = True,
 56 |     blocks: Optional[List[Dict[str, Any]]] = None,
 57 | ) -> requests.Response:
 58 |     if not SLACK_WEBHOOK_URL:
 59 |         raise RuntimeError("SLACK_WEBHOOK_URL is not set")
 60 | 
 61 |     payload = {
 62 |         "text": text,
 63 |         "unfurl_links": enable_unfurl,
 64 |         "unfurl_media": enable_unfurl,
 65 |     }
 66 |     if blocks:
 67 |         payload["blocks"] = blocks
 68 | 
 69 |     resp = requests.post(SLACK_WEBHOOK_URL, json=payload, timeout=10)
 70 |     resp.raise_for_status()
 71 |     return resp
 72 | 
 73 | 
 74 | def _extract_meta(content: str, pattern: str) -> str:
 75 |     """Return first capture from regex pattern (dotall)."""
 76 |     m = re.search(pattern, content, re.IGNORECASE | re.DOTALL)
 77 |     return (m.group(1).strip() if m else "").replace("\n", " ").strip()
 78 | 
 79 | 
 80 | def fetch_preview(url: str) -> Dict[str, str]:
 81 |     """
 82 |     Fetch page and extract og:title / og:description / title.
 83 |     Best-effort; returns empty strings on failure.
 84 |     """
 85 |     try:
 86 |         resp = requests.get(url, headers=HTTP_HEADERS, timeout=10)
 87 |         resp.raise_for_status()
 88 |         resp.encoding = resp.apparent_encoding or resp.encoding or "utf-8"
 89 |         html = resp.text
 90 |     except Exception:
 91 |         return {"title": "", "description": ""}
 92 | 
 93 |     og_title = _extract_meta(
 94 |         html, r'<meta[^>]+property=["\']og:title["\'][^>]+content=["\'](.*?)["\']'
 95 |     )
 96 |     og_desc = _extract_meta(
 97 |         html, r'<meta[^>]+property=["\']og:description["\'][^>]+content=["\'](.*?)["\']'
 98 |     )
 99 |     og_image = _extract_meta(
100 |         html, r'<meta[^>]+property=["\']og:image["\'][^>]+content=["\'](.*?)["\']'
101 |     )
102 |     title_tag = _extract_meta(html, r"<title[^>]*>(.*?)</title>")
103 | 
104 |     title = og_title or title_tag
105 |     description = og_desc
106 |     return {"title": title, "description": description, "image": og_image}
107 | 
108 | 
109 | def _source_label(url: str) -> str:
110 |     if "note.com" in url:
111 |         return "note（ノート）"
112 |     if "prtimes.jp" in url:
113 |         return "PR TIMES"
114 |     return "update"
115 | 
116 | 
117 | def _source_icon(url: str) -> str:
118 |     if "note.com" in url:
119 |         return "https://play-lh.googleusercontent.com/Jcdw4nXOdeg3pMPJldirClrj__oBd-UVZPehnb9Zn5MtWvWCQivgLqJ1mux0JjyxvA"
120 |     if "prtimes.jp" in url:
121 |         return "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTj6jiPk40UN8Vih6Vtaqv0DiIIF3ZUXWms1g&s"
122 |     return ""
123 | 
124 | 
125 | def send_with_preview(url: str) -> requests.Response:
126 |     """Send a Slack message with manual preview via blocks."""
127 |     meta = fetch_preview(url)
128 |     title = meta.get("title") or url
129 |     description = meta.get("description") or ""
130 |     image = meta.get("image") or ""
131 | 
132 |     if len(description) > 500:
133 |         description = description[:500] + "…"
134 | 
135 |     label = _source_label(url)
136 |     quoted_lines = []
137 |     quoted_lines.append(f"> {label}")
138 |     quoted_lines.append(f"> {url}")
139 |     quoted_lines.append(f"> *{title}*")
140 |     if description:
141 |         quoted_lines.append(f"> {description}")
142 | 
143 |     accessory = (
144 |         {"type": "image", "image_url": image, "alt_text": title or "preview"} if image else None
145 |     )
146 | 
147 |     blocks: List[Dict[str, Any]] = [
148 |         {
149 |             "type": "section",
150 |             "text": {"type": "mrkdwn", "text": "\n".join(quoted_lines)},
151 |             **({"accessory": accessory} if accessory else {}),
152 |         }
153 |     ]
154 | 
155 |     fallback_text = f"{label}: {title}"
156 |     return send_to_slack(fallback_text, enable_unfurl=False, blocks=blocks)
157 | 
158 | 
159 | def run_notification(window_hours: int = 1, *, persist_cache: bool = True) -> dict:
160 |     """
161 |     Execute release/note checks and post results to Slack.
162 |     Returns a summary dict for logging/HTTP responses.
163 |     """
164 |     load_spreadsheet_data(force_refresh=True, persist=persist_cache)
165 | 
166 |     pr_data = pr_checker.check_releases(window_hours=window_hours)
167 |     pr_checker.write_output(pr_data)
168 | 
169 |     note_data = note_checker.check_notes(window_hours=window_hours)
170 |     note_checker.write_output(note_data)
171 | 
172 |     pr_urls = [r.get("url", "") for r in pr_data.get("releases", []) if r.get("url")]
173 |     note_urls = [a.get("url", "") for a in note_data.get("articles", []) if a.get("url")]
174 | 
175 |     responses = []
176 |     for url in pr_urls:
177 |         responses.append(send_with_preview(url))
178 |     for url in note_urls:
179 |         responses.append(send_with_preview(url))
180 | 
181 |     if not pr_urls and not note_urls:
182 |         responses.append(send_to_slack("今回はありません"))
183 | 
184 |     summary = {
185 |         "pr_count": len(pr_urls),
186 |         "note_count": len(note_urls),
187 |         "messages_sent": len(responses),
188 |     }
189 |     return summary
190 | 
191 | 
192 | def main():
193 |     summary = run_notification(window_hours=1)
194 |     print(
195 |         f"Slack posted: pr_count={summary['pr_count']}, "
196 |         f"note_count={summary['note_count']}, "
197 |         f"messages_sent={summary['messages_sent']}"
198 |     )
199 | 
200 | 
201 | def build_response() -> Dict[str, Any]:
202 |     """Shared HTTP response builder for serverless/handler entrypoints."""
203 |     try:
204 |         summary = run_notification(window_hours=1)
205 |         body = {"ok": True, **summary}
206 |         status_code = 200
207 |     except requests.HTTPError as exc:
208 |         body = {"ok": False, "error": f"Slack HTTP error: {exc}"}
209 |         status_code = 502
210 |     except Exception as exc:
211 |         body = {"ok": False, "error": str(exc)}
212 |         status_code = 500
213 | 
214 |     return {
215 |         "statusCode": status_code,
216 |         "headers": {"Content-Type": "application/json"},
217 |         "body": json.dumps(body, ensure_ascii=False),
218 |     }
219 | 
220 | 
221 | class Handler(BaseHTTPRequestHandler):
222 |     """
223 |     Fallback handler for environments that expect BaseHTTPRequestHandler subclass.
224 |     Delegates to build_response above.
225 |     """
226 | 
227 |     def _dispatch(self):
228 |         result = build_response()
229 |         status = result.get("statusCode", 200)
230 |         headers = result.get("headers", {}) or {}
231 |         body = result.get("body", "")
232 |         if not isinstance(body, (str, bytes)):
233 |             body = json.dumps(body, ensure_ascii=False)
234 | 
235 |         self.send_response(status)
236 |         for k, v in headers.items():
237 |             self.send_header(k, v)
238 |         self.end_headers()
239 |         payload = body if isinstance(body, bytes) else body.encode("utf-8")
240 |         self.wfile.write(payload)
241 | 
242 |     def do_GET(self):
243 |         self._dispatch()
244 | 
245 |     def do_POST(self):
246 |         self._dispatch()
247 | 
248 | 
249 | # Alias expected by older Vercel Python runtimes: treat `handler` as a Handler subclass.
250 | handler = Handler
251 | 
252 | 
253 | if __name__ == "__main__":
254 |     main()
--------------------------------------------------------------------------------

/api/prtimes_notify.py:
--------------------------------------------------------------------------------
  1 | import json
  2 | import os
  3 | from http import HTTPStatus
  4 | from pathlib import Path
  5 | from typing import Any, Dict
  6 | 
  7 | import requests
  8 | 
  9 | # Vercel will handle SLACK_WEBHOOK_URL via env vars. For local runs, optional .env.
 10 | BASE_DIR = Path(__file__).resolve().parent.parent
 11 | ENV_PATH = BASE_DIR / ".env"
 12 | 
 13 | 
 14 | def _load_env_file():
 15 |     if not ENV_PATH.exists():
 16 |         return
 17 |     with ENV_PATH.open(encoding="utf-8") as f:
 18 |         for line in f:
 19 |             line = line.strip()
 20 |             if not line or line.startswith("#") or "=" not in line:
 21 |                 continue
 22 |             key, val = line.split("=", 1)
 23 |             val = val.strip().strip('"').strip("'")
 24 |             os.environ.setdefault(key, val)
 25 | 
 26 | 
 27 | _load_env_file()
 28 | 
 29 | # Import after env load so webhook is available if .env is present locally.
 30 | from api import b_send_slack  # noqa: E402
 31 | 
 32 | 
 33 | def handler(request) -> Dict[str, Any]:
 34 |     """
 35 |     Vercel Python entrypoint.
 36 |     Triggers PR TIMES + note checks and posts results to Slack.
 37 |     """
 38 |     try:
 39 |         summary = b_send_slack.run_notification(window_hours=1, persist_cache=False)
 40 |         body = {"ok": True, **summary}
 41 |         status = HTTPStatus.OK
 42 |     except requests.HTTPError as exc:
 43 |         body = {"ok": False, "error": f"Slack HTTP error: {exc}"}
 44 |         status = HTTPStatus.BAD_GATEWAY
 45 |     except Exception as exc:
 46 |         body = {"ok": False, "error": str(exc)}
 47 |         status = HTTPStatus.INTERNAL_SERVER_ERROR
 48 | 
 49 |     return {
 50 |         "statusCode": status.value,
 51 |         "headers": {"Content-Type": "application/json"},
 52 |         "body": json.dumps(body, ensure_ascii=False),
 53 |     }
 54 | 
 55 | 
 56 | if __name__ == "__main__":
 57 |     # Local manual trigger
 58 |     result = handler({})
 59 |     print(result)
--------------------------------------------------------------------------------

/dump.py:
--------------------------------------------------------------------------------
  1 | import os
  2 | import re
  3 | import fnmatch
  4 | 
  5 | def should_exclude(file_path):
  6 |     """
  7 |     指定されたファイルパスが除外対象かどうかを判定する
  8 |     """
  9 |     # 除外パターンのリスト
 10 |     exclude_patterns = [
 11 |         # Pythonバイトコード
 12 |         "__pycache__/*", "*.py[cod]", "*$py.class",
 13 |         # 環境設定ファイル
 14 |         ".env", "*.json", ".venv/*", "venv/*", "result/*", "ENV/*", "env.bak/*", "env/*",
 15 |         # pip環境設定ファイル
 16 |         "pip-log.txt", "pip-delete-this-directory.txt",
 17 |         # コンパイル成果物
 18 |         "*.egg-info/*", "*.egg", "*.eggs", "*.whl","*.git",".git",
 19 |         # テストカバレッジレポート
 20 |         "htmlcov/*", ".tox/*", ".nox/*", ".coverage", "coverage.*", ".cache",
 21 |         "nosetests.xml", "coverage.xml", "*.cover", "*.py,cover",
 22 |         # Jupyter Notebookのチェックポイント
 23 |         ".ipynb_checkpoints/*",
 24 |         # pylint, mypyなどの設定
 25 |         ".mypy_cache/*", ".pyre/*", ".pytype/*", ".pyright/*",
 26 |         # IDEやエディタの設定ファイル
 27 |         ".vscode/*", ".idea/*", "*.sublime-workspace", "*.sublime-project",
 28 |         # MacやLinuxのシステムファイル
 29 |         ".DS_Store", "*.swp", "*~",
 30 |         # パッケージ管理ツールの成果物
 31 |         "poetry.lock", "Pipfile.lock",
 32 |         # Docker関連
 33 |         "docker-compose.override.yml", ".dockerignore",
 34 |         # その他
 35 |         "*.log", "*.pot", "*.mo", "cline_log.txt", "git_tracking_status.txt",
 36 |         # 本番環境用秘密ファイル
 37 |         "*.pem", ".secrets", ".env.act",
 38 |         # 出力ファイル自体を除外
 39 |         "dump_result.txt"
 40 |     ]
 41 | 
 42 |     # ファイル名のみを取得
 43 |     file_name = os.path.basename(file_path)
 44 | 
 45 |     # ディレクトリパスを含む相対パス
 46 |     rel_path = file_path
 47 | 
 48 |     # 除外パターンとマッチするかチェック
 49 |     for pattern in exclude_patterns:
 50 |         # ファイル名だけでマッチングを試みる
 51 |         if fnmatch.fnmatch(file_name, pattern):
 52 |             return True
 53 | 
 54 |         # パスを含めたマッチングも試みる
 55 |         if fnmatch.fnmatch(rel_path, pattern):
 56 |             return True
 57 | 
 58 |         # __pycache__ ディレクトリ内のファイルを除外
 59 |         if "__pycache__" in rel_path:
 60 |             return True
 61 | 
 62 |     # .git 配下は丸ごと除外
 63 |     if rel_path.startswith(".git") or "/.git/" in rel_path:
 64 |         return True
 65 | 
 66 |     return False
 67 | 
 68 | def dump_files_to_txt(target_dir):
 69 |     """
 70 |     指定されたディレクトリ内のファイルを走査し、内容をdump_result.txtに出力する
 71 |     除外リストに含まれるファイルはスキップする
 72 |     フォルダとファイルはアルファベット順にソートされる
 73 |     """
 74 |     # 出力ファイルのパスを指定されたディレクトリ内に設定
 75 |     output_file = os.path.join(target_dir, "dump_result.txt")
 76 | 
 77 |     # 相対パスの基準となるディレクトリ
 78 |     base_dir = target_dir
 79 | 
 80 |     # ファイルパスを収集してソートする
 81 |     all_files = []
 82 |     for root, dirs, files in os.walk(target_dir):
 83 |         # .git など除外ディレクトリを辿らないようフィルタ
 84 |         dirs[:] = [d for d in dirs if not should_exclude(os.path.relpath(os.path.join(root, d), start=base_dir))]
 85 |         # ディレクトリをアルファベット順にソート
 86 |         dirs.sort()
 87 |         # ファイルをアルファベット順にソート
 88 |         for fname in sorted(files):
 89 |             abs_path = os.path.join(root, fname)
 90 | 
 91 |             # 出力ファイル自体はスキップ
 92 |             if abs_path == output_file:
 93 |                 continue
 94 | 
 95 |             # 除外対象のファイルはスキップ
 96 |             rel_path = os.path.relpath(abs_path, start=base_dir)
 97 |             if should_exclude(rel_path):
 98 |                 continue
 99 | 
100 |             all_files.append((rel_path, abs_path))
101 | 
102 |     # ファイルをアルファベット順にソート
103 |     all_files.sort()
104 | 
105 |     with open(output_file, "w", encoding="utf-8") as out:
106 |         for rel_path, abs_path in all_files:
107 |             out.write(f"/{rel_path}:\n")
108 |             out.write("-" * 80 + "\n")
109 |             try:
110 |                 with open(abs_path, "r", encoding="utf-8") as f:
111 |                     for i, line in enumerate(f, 1):
112 |                         out.write(f"{i:3} | {line.rstrip()}\n")
113 |             except Exception as e:
114 |                 out.write(f"[ERROR READING FILE]: {e}\n")
115 |             out.write("-" * 80 + "\n\n")
116 | 
117 | if __name__ == "__main__":
118 |     # コマンドライン引数を使わず、直接変数に値を設定
119 |     # target_directory = "/Users/neromehiro/hiro folder/my_Works/programing/aimsales-api/module/b_AddCompany"
120 |     target_directory = "/Users/neromehiro/hiro folder/my_Works/programing/makers_release"
121 |     dump_files_to_txt(target_directory)
122 |     print(f"✅ 出力完了: {target_directory}/dump_result.txt")
--------------------------------------------------------------------------------

/requirements.txt:
--------------------------------------------------------------------------------
  1 | requests
--------------------------------------------------------------------------------

/spreadsheet2json.py:
--------------------------------------------------------------------------------
  1 | """
  2 | Fetches public Google Sheet data and writes prtimes_id / note_id lists to JSON.
  3 | 
  4 | The target sheet is shared read-only, so we can download it as CSV via the
  5 | `export?format=csv` endpoint. IDs marked as 'なし' or left blank are skipped.
  6 | """
  7 | 
  8 | from __future__ import annotations
  9 | 
 10 | import csv
 11 | import json
 12 | from pathlib import Path
 13 | from typing import Iterable, List, Mapping
 14 | from urllib.parse import urlparse
 15 | 
 16 | import requests
 17 | 
 18 | 
 19 | SHEET_ID = "16zcRbmvBEdkWDARh-OZ_r8b-ucnKoFKQNLp9nF3EV4k"
 20 | SHEET_GID = "0"
 21 | CSV_URL = (
 22 |     f"https://docs.google.com/spreadsheets/d/{SHEET_ID}/export?format=csv&gid={SHEET_GID}"
 23 | )
 24 | OUTPUT_PATH = Path("spreadsheet2json.json")
 25 | 
 26 | 
 27 | def _dedupe_preserve_order(values: Iterable[str]) -> List[str]:
 28 |     seen = set()
 29 |     unique = []
 30 |     for value in values:
 31 |         if value not in seen:
 32 |             seen.add(value)
 33 |             unique.append(value)
 34 |     return unique
 35 | 
 36 | 
 37 | def _is_missing(value: str) -> bool:
 38 |     normalized = value.replace("　", "").strip()
 39 |     return bool(
 40 |         not normalized
 41 |         or normalized in {"なし", "ナシ", "無し"}
 42 |         or normalized.startswith(("なし", "ナシ", "無し"))
 43 |     )
 44 | 
 45 | 
 46 | def _normalize_note_id(value: str) -> str:
 47 |     """Return just the note username/slug from a URL or raw value."""
 48 |     raw = value.strip()
 49 |     if not raw:
 50 |         return ""
 51 | 
 52 |     # Accept URLs such as https://note.com/username or note.com/@username
 53 |     if "note.com" in raw or "://" in raw:
 54 |         parsed = urlparse(raw if "://" in raw else f"https://{raw}")
 55 |         path = parsed.path.strip("/")
 56 |         if not path:
 57 |             return ""
 58 |         raw = path.split("/")[0]
 59 | 
 60 |     # Strip leading @ if present (note profile pages sometimes include it)
 61 |     return raw.lstrip("@")
 62 | 
 63 | 
 64 | def fetch_sheet_csv() -> str:
 65 |     resp = requests.get(CSV_URL, timeout=30)
 66 |     resp.raise_for_status()
 67 |     resp.encoding = "utf-8"
 68 |     return resp.text
 69 | 
 70 | 
 71 | def parse_ids(csv_text: str) -> dict:
 72 |     prtimes_ids = []
 73 |     note_ids = []
 74 |     x_ids = []
 75 | 
 76 |     reader = csv.DictReader(csv_text.splitlines())
 77 |     for row in reader:
 78 |         prtimes_value = (row.get("prtimes_id") or "").strip()
 79 |         note_value = (row.get("note_id") or "").strip()
 80 |         x_value = (row.get("x_id") or "").strip()
 81 | 
 82 |         if not _is_missing(prtimes_value):
 83 |             prtimes_ids.append(prtimes_value)
 84 |         if not _is_missing(note_value):
 85 |             normalized = _normalize_note_id(note_value)
 86 |             if normalized:
 87 |                 note_ids.append(normalized)
 88 |         if not _is_missing(x_value):
 89 |             x_ids.append(x_value)
 90 | 
 91 |     return {
 92 |         "prtimes_id": _dedupe_preserve_order(prtimes_ids),
 93 |         "note_id": _dedupe_preserve_order(note_ids),
 94 |         "x_id": _dedupe_preserve_order(x_ids),
 95 |     }
 96 | 
 97 | 
 98 | def write_json(data: dict) -> None:
 99 |     text = json.dumps(data, ensure_ascii=True, indent=2)
100 |     try:
101 |         OUTPUT_PATH.write_text(text, encoding="utf-8")
102 |     except OSError:
103 |         # Skip persisting when filesystem is read-only (e.g., serverless)
104 |         pass
105 | 
106 | 
107 | def load_spreadsheet_data(
108 |     force_refresh: bool = False, *, persist: bool = True
109 | ) -> Mapping[str, List[str]]:
110 |     """
111 |     Return parsed sheet data, optionally re-fetching the latest CSV.
112 | 
113 |     Falls back to re-fetching if the on-disk JSON is missing or invalid.
114 |     When persist=False (e.g., API on read-only FS), skip writing the JSON cache.
115 |     """
116 |     if not force_refresh and OUTPUT_PATH.exists():
117 |         try:
118 |             return json.loads(OUTPUT_PATH.read_text(encoding="utf-8"))
119 |         except Exception:
120 |             pass
121 | 
122 |     csv_text = fetch_sheet_csv()
123 |     data = parse_ids(csv_text)
124 |     if persist:
125 |         write_json(data)
126 |     return data
127 | 
128 | 
129 | def main() -> None:
130 |     csv_text = fetch_sheet_csv()
131 |     data = parse_ids(csv_text)
132 |     write_json(data)
133 | 
134 | 
135 | if __name__ == "__main__":
136 |     main()
--------------------------------------------------------------------------------

